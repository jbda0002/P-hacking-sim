\section{Discussion}
There is an increasing awareness among scientists that we need to  lower the number of false positive findings to increase the replicability of research. One way to reduce false positive findings is with preregistration of a single data plan for pre-processing and analysis since this removes researcher flexibilities that can otherwise inflate the risk of false positives \citep{Simmons2011}. However, even with a plan for pre-processing and analysis there is still a risk that the FPP may be higher than 5\%. Simulation research shows that type I error rates can increase for several reasons such as model misspecification, noise or ceiling effects in independent variables, or dichotomization of continuous covariates \citep{Dennis2019, Litiere2007, Brunner2009, Austin2003, Austin2004}. To better understand the effectiveness of preregistration on FPP and FPR, we used a simulation to examine how FPP and FPR were affected by different flexibilities. Specifically, we looked into the following flexibilities: constructing a model set with several covariates, using multiple outlier criteria, and collecting multiple dependent variables. We also examined how the precision of the estimates (a larger sample) and the correlations between the dependent variable and the covariates affected the ability to find a significant effect both when the study would be labeled as exploratory but also under the criteria of preregistration. \\

In general, preregistration helps a lot with lowering the risk of finding a significant result. However, not in all cases is the preregistration itself sufficient to lower the FPR to the desired 5\%. In particular, the preregistration does not help when the main effects are not included in the model and the covariates are dummy coded. In this case the FPR can reach 42\%, meaning that on average 42\% of the models in a given set have a significant effect (either the interaction or the main effect). This happens due to the fact that the interaction effect captures the true effect that would have been in the main effect of the covariate. So even though some literature still suggests that one can exclude the main effect when doing interactions as long as it is theory based, there seems to be no good reason for this when the covariates are dummy coded. When the variables are effects coded, the FPR decreases, but it is still above a FPR of 5\%. The only way to overcome this issue is to always include the main effects when the interaction effects are of interest. However, even then, the FPR will still be slightly higher than 5\%. This is due to the fact that multiple tests are performed at at the same time. A simple solution to overcome this issue is to correct the p-values by  using a correction such as Bonferroni correction \citep{dunn1961multiple} (see Figure S1).  \\

Given that it is possible to get a FPP of 100\%, labeling research that goes further than the preregistered analysis as exploratory can have little to no meaning if using p-values. Even with performing correct analyses as including all the main effects when doing interactions and correcting p-values to follow the number of tests in a given model, the FPP is still higher than 5\%. P-values are therefore not a good indicator for if a effect is present if the analysis is exploratory. This is not to say that exploratory research does not have a place, but using p-values might not be a good indicator for if an exploratory analysis is of interest. \\
    
It has been suggested that one way to lower the FPP would be to ensure a big enough sample \citep{Simmons2011}. Even though this recommendation has been eased, the authors still make the same suggestion: increasing the sample size could help with lowering the FPP \cite{simmons2018}. This is, however, not a reasonable way to lower the FPP or the FPR. Even worse, in combination with bad practices (e.g. leaving out main effects when using interactions), having a large sample can increase the FPP. This recommendation may seem counter-intuitive. But what is investigated in this paper is a true null effect. If there was a true effect then a bigger sample is needed as small samples have a risk to revert the sign of the effect or exaggerate the effect \citep{gelman2014beyond}. We therefore do not recommend to run low powered studies but just remind that just have a big enough sample does not mean that the FPP and FPR will decrease.\\

There are, of course, factors that are not under the control of the researcher, such as the correlation between the covariates. But it is still worth a mention, that the higher the correlation between the covariates and the dependent variable, the easier it is to find a significant model. This is the case for both exploratory analyses and for a preregistered analysis. However, it is less clear how to solve this problem. Study designs with common method bias (e.g., measuring all covariates in one survey) could be particularly problematic due to  the increased correlation between the covariates \citep{podsakoff2003}. A potential remedy could be to report correlation matrices which would allow readers to assess the inflated risk of false positive findings.  \\ 

As expected, using several flexibilities increases the risk of finding a model with a significant effect even though no true effect is present. However, preregistration does not seem to be enough to bring the FPR down to the desired level of 5\%. We therefore made the following recommendations to reduce the probability of getting a false positive result. 

\subsection{Guidelines for researchers}

\subsubsection{Follow the principles of open science and use preregistration}
This reduces the number of researcher flexibilities and is therefore the most important step towards reducing the FPP. Researchers should therefore follow the already developed guidelines \citep{Nosek2015}. This means that research should be preregistered and all raw data and analyses code should be made available for other researchers to make it easier to see if any mistakes or mentioned flexibilities occurred during the analysis. 
\subsubsection{Use Bonferroni corrections when looking at interactions.}
A Bonferroni correction should be used when running regressions with interaction effects . If this is not done, the FPR will be higher than 5\%, and therefore the FPP would also be higher than 5\%. In some cases the correction can lower the FPR to less than 5\%, but we believe that this is a better outcome than having one that is higher than 5\%. 
\subsubsection{Large samples do not protect against FPP and FPR}
Having a large enough sample does not legitimize exploratory analyses. In general, the sample size only affects the power of the study and the precision of the estimates, it does not remedy the FPR and FPP. 

\subsection{Guidelines for reviewers and editors}

\subsubsection{Look for correction of p-values}
If multiple comparisons were made or interaction effects were used, there should be a correction of the p-values.
\subsubsection{Require that main effects always follow interaction effects}
As shown, a reviewer should never accept an interaction effect to be true if the main effect are not present and the variables are dummy coded. 

\subsection{Additional contributions}
The size of a given model set and its permutations may not only be of interest to the open science community. Our description of model sets could also be of interest to researchers that use machine learning as a model selection tool. Since researchers who work with machine learning rarely care about one specific variable but rather about the predictability of a general model, using the sets where HCI is included is not always meaningful. Instead, what matters is whether the researcher will allow for interactions between the independent variables. The calculations of the model sets are therefore the same as if there was a variable of interest, but excluding the set that has HCI in it. This means that with the requirement that the main effect should follow along with the interactions, the sets of interest are Ma and Ma + CCI. With no such requirements, the sets of interest are Ma, CCI and Ma + CCI. The number of models with and without restrictions, different number of variables, and how to calculate them can be found in Table SI9 and Table SI10. 


 
