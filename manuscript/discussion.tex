\section{Discussion}
There is a growing awareness among researchers of the need to lower the number of false-positive results reported in the scientific literature. One way to reduce false-positive results is by preregistering a single data plan for pre-processing and analysis since this removes researcher flexibilities that can otherwise inflate the risk of false-positives \citep{simmons2018}. However, there is an ongoing discussion about the effectiveness of this procedure (see e.g. \cite{Pham2020} and \cite{Simmons2020}). While researcher flexibilities such as the use of outlier criteria or multiple dependent variables are often featured as the main cause of false-positive results \citep{John2012}, previous research has shown that conditions such as model misspecification, noise or ceiling effects in the independent variables, or dichotomization of continuous covariates may also lead to type I error rates beyond 5\% \citep{Dennis2019, Brunner2009, Austin2003, Austin2004,Litiere07}. \\

Using a simulation approach, we examine the role of various data structure and data analysis conditions in generating false-positive results: constructing a model set with several covariates, allowing for interactions with and without main effects being present, using different sample sizes, and the strength of correlations between the dependent variable and the covariates. We examine the effect of these conditions on the false-positive probability (FPP) i.e., the chance of erroneously finding one or more significant effects among all the tested models, and on the false-positive ratio (FPR) i.e., the ratio of  false-positive models to all tested models. The FPP tells us what the probability of a false-positive result is if a researcher tests all models and flexibilities. This metric is informative about exploratory analyses that is, when there is no clear plan beforehand regarding what should be analysed. The FPR tells us about the probability of a false-positive result if a researcher only tests one model under certain conditions. This means that the FPR can be seen as the risk of a false-positive result for researchers that preregister their analysis plan. \\

Our findings show that in many cases the FPR lies in the vicinity of the expected 5\%, which is an indicator that preregistering and running a single analysis is generally very helpful for lowering the risk of obtaining a false-positive result. However, preregistration is not always sufficient to reach the desired 5\%. In particular, preregistration is less effective when the preregistered model includes interactions without main effects and especially when covariates are binary and dummy coded. In this case, the FPR reaches over 34 \%. This happens due to the fact that the interaction effect captures the true effect of the covariate that would have otherwise been expressed as a significant main effect of the covariate, i.e. we get a misspecified model with a biased estimate. Although some authors suggests that one can omit main effects in models with interaction effects when it is theoretically justified, there is a clear statistical reason not to do this, especially when covariates are binary and dummy coded. One way to mitigate this issue is to always include main effects when interaction effects are of interest. However, even so, the FPR will still be slightly higher than 5\%. This is due to the fact that multiple tests are performed at the same time. This may then require an adjustment of the p-value such as Bonferroni correction \citep{dunn1961multiple}. Our calculations show that a Bonferroni correction can in fact remedy the issue and bind the FPR at the level 0.05 regardless of the model specification (see \textit{Supplementary Information} in section \nameref{resultBC}).  \\

Given the ease with which one can obtain an FPP of 100\% the usefulness of significance tests in exploratory analyses is strongly reduced. Even when correctly performing analyses such as including all corresponding main effects to interaction effects and using Bonferroni correction to obtain a desired FPR, the FPP is still higher than 5\% (see Figure \ref{fig:mainfigure1} with restrictions on main effects). A standard approach to significance testing is therefore problematic for determining the presence of an effect if the analysis is exploratory. An alternative could be a more severe threshold for significance (see suggestions by \cite{benjamin2018}) or focusing on effect sizes rather than significance levels. However, we do not recommend any conclusions about an effect being true or not in any exploratory analysis using p-values.  
    
It has been suggested that one way to lower the FPP is to ensure sufficient statistical power by having enough observations per cell to detect a true effect \citep{Simmons2011, simmons2018}. However, our simulations show that this is not an effective way to lower neither the FPP nor FPR. In most cases, the sample size does not affect either the FPP or FPR, but under some conditions, such as when leaving out main effects when using interactions (having biased estimates), larger samples can actually increase the FPP and FPR. This may seem counter-intuitive. However, we should bear in mind that here we investigate how often a true null effect can become significant. In this case, if there is no bias from the estimate and the models are correctly specified, the distribution of the p-values would be uniformed if only one variable is tested and therefore not affected by the increase in sample size \citep{Murdoch2008}. In a situation where there is a true effect, a larger sample is indeed necessary as small samples can produce exaggerated effects or even effects in the opposite direction \citep{gelman2014beyond}. In  cases where there is a true effect the p-values would be normally distributed with a sufficient large sample. Here the issue would be of having a small sample and a low correlation between the dependent variable and the variable of interest as this again would lead to a uniform distribution. Naturally, we do not recommend using small sample sizes, but simply point out that having a large sample does not guarantee an FPP or FPR of 5\%. \\

We also tested the influence of the correlation structure between the dependent variable and covariates. The true correlation structure is often unknown to the researcher, but can in some cases be inferred from prior literature using similar measures. We find that the higher the correlation between the covariates and dependent variable, the easier it is to find a model with a false-positive result both in a single preregistered analysis and exploratory analyses. However, this only holds for models with interaction effects between the variable of interest and covariates. Study designs with common method bias i.e., that measure all covariates in one survey, could be particularly problematic due to the increased correlation between the variables \citep{podsakoff2003}. A potential remedy could be to report correlation matrices which would allow readers to assess whether there is an increased risk of false-positive findings as higher correlation between the covariates and the dependent variable could lead to higher risk of false positive if there is interactions between the variable of interest and the covariates.  \\ 

To summarize, our findings reinforce some well-known measures against false-positive results, but also suggest several new ones. First and foremost researchers should follow guidelines for preregistration \citep{Nosek2015,VANTVEER20162} which reduces the number of researcher flexibilities such as the use of outlier criteria, multiple dependent variables, or testing multiple models. Second, Bonferroni corrections should be used when running regressions with interaction effects to avoid having the FPP and FPR higher than 5\%. Third, when testing interaction effects models should always include the corresponding main effects. Fourth, having a large sample does not legitimize exploratory analyses. In general, the sample size only affects the power of the study and the precision of the estimates, it does not remedy the FPP and FPR. Fifth, researchers should not draw any conclusions based on significance tests in exploratory analyses. Effects should be gauged using other measures of statistical importance or alternatively alpha levels should be set at a very conservative level. \\

Researchers who wish to go against these recommendations for theoretical or other reasons are advised to show by simulation that their analysis does not have an inflated risk of generating false-positive results or if finding that it does have a higher risk, adjusting the alpha level to mitigate the issue. In general, we believe it would be useful always to supplement power analyses with Type I error analyses similar to the ones reported here. The theoretical assumption in power analysis is that Type I error rates are determined by the alpha level, but our findings show that this is often not the case.


%The size of a given model set and its permutations may not only be of interest to the open-science community. Our description of model sets could also be of interest to researchers that use machine learning as a model-selection tool. Since researchers who work with machine learning rarely care about one specific variable but rather about the predictability of a general model, using the sets where HCI is included is not always meaningful. Instead, what matters is whether a researcher will allow for interactions between the independent variables. The calculations of the model sets are therefore the same as if there was a variable of interest, but excluding the sets that have HCI in it. This suggests that considering the restriction that main effects should always follow along with interactions, the sets of interest here are ME and ME + CCI. With no such restriction, the sets of interest are ME, CCI, and ME + CCI. The number of models with and without restrictions, different number of variables, and how to calculate them can be found in Table S9 and Table S10. 


 
