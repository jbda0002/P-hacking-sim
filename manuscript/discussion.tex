\section{Discussion}
There is an increasing awareness among scientists that we need to increase the replicability of research by lowering the number of false positive findings that get published. One way to reduce false positive findings is with preregistration of a single data plan for pre-processing and analysis since this removes researcher flexibilities that can otherwise inflate the risk of false positives \citep{Simmons2011}. However, even with a plan for pre-processing and analysis there is a risk that the false positive probability may be higher than 5\%. Simulation research show that type I error rates can increase for several reasons such as model misspecification, noise or ceiling effects in independent variables, or dichotomization of continuous covariates \citep{Dennis2019, Litiere2007, Brunner2009, Austin2003, Austin2004}. To better understand the effectiveness of preregistration on false positive rates we used a simulation to examine how FPP and FPR were affected by different flexibilities. Specifically, we looked into the following flexibilities; constructing a model set with several covariates, using multiple outlier criteria, and collecting different dependent variables. We also examined how the precision of the estimates (a bigger sample) and the correlations between the dependent variable and the covariates affected the ability to find a significant effect both when the study would be labeled as exploratory but also under the criteria of preregistration. \\

In general preregistration helps a lot with lowering the risk of finding a significant result. However, it is not enough to lower the FPR to 5\%  in all cases. This happens when main effects are not included and the covariates are dummy coded. In this case the FPR can exceed 35\%, meaning that on average 35\% of the models in a given set had a significant effect (either the interaction or the main effect). This happens due to the fact that the interaction effect captures the true effect that would have been in the main effect of the covariate. So even though some literature still suggest that one can exclude the main effect when doing interactions as long as it is theory based, there seems to be no good reason for this as long as the covariates are dummy coded. This also means that the FPR falls as soon as the variable is effects coded, but it is still above a FPR of 5\%. The only way to overcome this issue is to always include the main effect when interaction effects are of interest. However, even in the sets that include the main effects and interactions the FPR will still be slightly higher than 5\%. This is due to the fact that multiple test's are performed at at the same time. A simple solution to overcome this issue is to correct the p-values for multiple tests, such as using Bonferroni correction \citep{dunn1961multiple} (See figure SI1 in supplementary information).  \\

Given that it is possible to get a FPP of 100\%, labeling research that goes further than the preregistered analysis as exploratory can have little to no meaning. Even with performing correct analyses as including all the constitutive terms and correcting p-values to follow the number of tests in a given model the FPP is still higher than 5\%. P-values are therefore not a good indicator for an effect if the analysis is exploratory. \\
    
It has been suggested that one way to lower the FPP would be to ensure a big enough sample \citep{Simmons2011}. Even though this recommendation has been softened a bit, the authors still make the same suggestion: increase the sample could help with lowering the FPP \cite{simmons2018}. This is however not a reasonable way to lower the FPP or the FPR. And even worse, in combination with bad practices, having a large sample can increase the FPP. It is still the case that low power can increase the effect size of non significant results \citep{loken2017measurement}, but expecting that it will lower the FPP and FPR is not the case for any model set. \\

There is, of course, factors that are not under the control of the researcher, such as the correlation between the covariates. But it still is worth a mention, that the higher correlated the variables are, the easier it is to find a significant model, this is the case both for exploratory analyses and for a preregistered analysis. How to solve this problem, is however not as clear as the other issues. One reason for concern might be study designs with common method bias, e.g. measuring all covariates in one survey which can increase the correlation between them \citep{podsakoff2003}. A possible remedy could be to report correlation matrices which would allow readers to assess the risk of inflated risk of false positive findings.  \\ 

As one would expect using several flexibilities increases the risk of finding a significant model even though no effect is present. However, preregistration does not seem to be enough to bring the false positive rate down to the desired level of 5\%. We therefore make the following recommendations to lower the chance of getting a false positive result. 

\subsection{Guidelines for researchers}

\subsubsection{Follow the principles of open science and use preregistration}
This reduces the number of researcher flexibilities and is the most important step towards lowing the FPP. Researchers should therefore follow the guidelines already developed \citep{Nosek2015}. This means that research should be preregistered and all data and code should be made available for other researchers to make it easier to see if any mistakes occurred during the analysis. 
\subsubsection{Use Bonferroni corrections when looking at interactions.}
When running regressions with interaction effects a Bonferroni correction should be used. If this is not done the FPR will be higher than 5\%, and therefore the FPP would also be higher than 5\%. In some cases the correction can lower the FPR to less than 5\%, but we believe that this is a better outcome than having one that is higher than 5\%. 
\subsubsection{Large samples does not protect against FPP and FPR}
Having a large enough sample do not legitimize exploratory analyses. In general, the sample size only affects the power of the study and the precision of the estimates, it does not remedy the FPR and FPP. 

\subsection{Guidelines for reviewer and editors}

\subsubsection{Look for correction of p-values}
If multiple comparisons are being made or interaction effects are used, there should be a correction of the p-values.
\subsubsection{Demand main effects follow interaction effects}
If the main effect is not included when interaction effects are investigated there should be a thorough explanation for why this is the case.

\subsection{Additional contributions}
The size of a given model set and its permutations may not only be of interest to the open science community. Our description of model sets could also be of interest to researchers that use machine learning as a model selection tool. Since researchers who work with machine learning rarely care about one specific variable but rather about the predictability of a general model, using the sets where HCI is included is not always meaningful. Rather it matters if the researcher will allow for interactions between the independent variables. The calculation of the model sets are therefore the same as if there is a variable of interest but excluding the set that has HCI in it. This means that under restrictions that the main effect should follow along with the interactions the sets are Ma and Ma + CCI. With no restrictions for the main effects to be present when doing interactions the sets are Ma, CCI and Ma + CCI. The number of models with and without restrictions with different numbers of variables and how to calculate them can be found in Table SI9 and Table SI10 in the supplementary information. 


 
