\section{Discussion}
There is an increasing awareness among researchers that we need to lower the number of false-positive findings to increase the replicability of research. One way to reduce false-positive findings is by preregistering a single data plan for pre-processing and analysis since this removes researcher flexibilities that can otherwise inflate the risk of false positives \citep{Simmons2018}. However, in this article we showed that even with a plan for pre-processing and analysis, there is still a risk of the FPP being higher than expected 5\%. Previous (simulation) research has shown that type I error rates can increase for several reasons such as model misspecification, noise or ceiling effects in independent variables, or dichotomization of continuous covariates \citep{Dennis2019, Litiere2007, Brunner2009, Austin2003, Austin2004}. To better understand the consequences of using such researcher flexibilities on the rates of false-positive findings, we used a simulation approach to examine how both the FPP (i.e., the chance of erroneously finding one or more significant effects among all the tested ones) and FPR (i.e., the number of models with false-positive effect within any given set of models) were affected by the use of different flexibilities. Specifically, we looked into the following flexibilities: constructing a model set with several covariates, allowing for interactions with and without main effects being present, using multiple outlier criteria, collecting multiple dependent variables, and using different sample sizes. We also examined how the correlations between the dependent variable and the covariates and different data structures affected the ability to find a significant effect. \\

When focusing on the FPP and FPR as a measurement of the consequences of using different flexibilities, one can notice that the two are related but their contributions differ. The FPP tells us what the probability of a false-positive result is if a researcher tests all models and flexibilities. This measurement is important when we are performing exploratory analysis that is, when there is no clear plan beforehand regarding what should be analysed. On the other hand, the FPR tells us about the probability of a false-positive result if a researcher is unsure about which model to use and thus randomly chooses and tests one of the models from a given model set. This suggests that the FPR can be seen as a risk of a false-positive result for researchers that preregister their analysis plan, but where in many cases it is not clear beforehand which outlier criteria to use or how the different variables should enter the model (e.g., only as main effects or also as interactions).
It should be stated that when we talk about preregistration here, we talk about a preregistration in a perfect form, meaning that it is clear beforehand what is being analysed, what the variables of interest are, and which variables one needs to control for. However, this is not always clear from the beginning and these decisions can have an influence on the outcome of the analysis \citep{Bryan25535,gilbert2016comment}.\\ 

Our findings show that in many cases the FPR lies in the vicinity of expected 5\%, which is an indicator that preregistration is generally very helpful regarding lowering the risk of obtaining a false-positive result. However, not in all cases is the preregistration itself sufficient to lower the FPR to the desired 5\%. In particular, the preregistration does not help when main effects are not included in the model and especially when covariates are binary and dummy coded. In this case, the FPR can reach 31.9\%, meaning that on average 31.9\% of the models in a given model set contain a false-positive effect (either the interaction or the main effect). This happens due to the fact that the interaction effect captures the true effect of the covariate that would have otherwise been expressed as the significant main effect of the covariate. So even though some literature still suggests that one can exclude main effects when having interactions as long as it is theory based, there seems to be no good reason for this when the covariates are binary and dummy coded. When the variables are effects coded, the FPR decreases, but it is still above 5\%. The only way to overcome this issue is to always include main effects when interaction effects are of interest. However, even then, the FPR will still be slightly higher than 5\%. This is due to the fact that multiple tests are performed at the same time. A simple solution to overcome this issue is to correct the p-values by using a correction such as Bonferroni correction (see Figure S6) \citep{dunn1961multiple}. \\

Given that it is possible to obtain an FPP of 100\% even when preregistering one's research plan by preforming additional exploratory analyses, labeling analyses that go beyond the main analysis as exploratory can have little to no meaning if using p-values. Even when performing correct analyses such as including all of the main effects when doing interactions and correcting p-values to follow the number of tests in a given model, the FPP is still higher than 5\% (see Figure 1 where Main = True). P-values are therefore not a good indicator for determining the presence of an effect if the analysis is exploratory. We do not wish to say that exploratory research does not have its value, but simply that there should not be made any conclusions when having exploratory analysis, this should only be done on pre-registered analysis. \\
    
It has been suggested that one way to lower the FPP would be to ensure having a well-powered study, such that there is enough observations per cell to detect a true effect \citep{Simmons2011, simmons2018}. This is, however, not an effective way to lower the FPP nor FPR since, in combination with bad practices (e.g., leaving out main effects when using interactions), larger samples can produce higher rates of the FPP. This may seem counter-intuitive; however, the reader should bear in mind that in this article we investigated how often a true null effect can become significant. In a situation where there is a true effect, a bigger sample is indeed needed as small samples can produce exaggerated effects or effects in the opposite direction to the true direction \citep{gelman2014beyond}. We do not recommend to run studies with small sample sizes, but simply remind that just having a big enough sample does not mean that the FPP and FPR will be bound to 5\%. \\

There are, of course, factors that are not under the control of a researcher, such as the correlation between the covariates and the dependent variable. But it is still worth a mention, that the higher the correlation between the covariates and the dependent variable, the easier it is to find a model with a false-positive effect. This is the case for both exploratory analyses and a preregistered analysis. However, it is less clear how to solve this problem. Study designs with common method bias (e.g., measuring all covariates in one survey) could be particularly problematic due to the increased correlation between the covariates \citep{podsakoff2003}. A potential remedy could be to report correlation matrices which would allow readers to assess the inflated risk of false-positive findings. \\ 

To sum up, using several researcher flexibilities increases the risk of finding a model with a significant effect even though no true effect is present. However, preregistration does not seem to be enough to bring the FPP and FPR down to the desired level of 5\%. We therefore made the following recommendations to reduce the probability of getting a false-positive result. 

\subsection{Guidelines for researchers}

\subsubsection{Follow the principles of open science and use preregistration}
This reduces the number of researcher flexibilities and is therefore the most important step towards reducing the FPP. Researchers should therefore follow the already developed guidelines \citep{Nosek2015}. This means that research should be preregistered and all raw data and analysis code should be made available; this would help other researchers to more easily see if any mistakes occurred or previously mentioned flexibilities were used during the analysis. 
\subsubsection{Use Bonferroni corrections when looking at interactions.}
A Bonferroni correction should be used when running regressions with interaction effects. If this is not done, the FPR will be higher than 5\%, and therefore the FPP would also be higher than 5\%. In some cases the correction can lower the FPR to less than 5\%, but we believe that this is a better outcome than having one that is higher than 5\%. 
\subsubsection{Large samples do not protect against the FPP and FPR}
Having a large enough sample does not legitimize exploratory analyses. In general, the sample size only affects the power of the study and the precision of the estimates, it does not remedy the FPR and FPP. 
\subsubsection{Do not make conclusions on exploratory analysis}
There should not be made any conclusions on exploratory analysis. This does not mean that exploratory analysis does not have is place within research rather that conclusions should only be made on pre-registered analysis. The exploratory analysis can still give foundation for discussion and further research. 

\subsection{Guidelines for reviewers and editors}

\subsubsection{Make sure data and analysis is available}
If there is made any hard conclusions in the paper it is important that these hypothesis has been registered beforehand and the data is available for other researchers to check the result. 
\subsubsection{Look for correction of p-values}
If multiple comparisons were made or interaction effects were used, there should be a correction of the p-values. We here suggest Bonferroni correction. We know that this can have a over compensation but we think that the cost is higher for a false positive than a false negative. 
\subsubsection{Require that main effects always follow interaction effects}
A reviewer should never accept an interaction effect to be genuine if main effects are not present and the variables are binary and dummy coded. 
\subsubsection{Exploratory analysis}
If exploratory analysis has been performed, there should never be drawn any conclusions from it if the measurement of the presence of an effect is a p-value. Exploratory analysis still has a place within research, but drawing conclusions from it is a bad practice if one relies solely on p-values and no follow-up replication studies have been performed.  

\subsection{Additional contributions}
The size of a given model set and its permutations may not only be of interest to the open-science community. Our description of model sets could also be of interest to researchers that use machine learning as a model-selection tool. Since researchers who work with machine learning rarely care about one specific variable but rather about the predictability of a general model, using the sets where HCI is included is not always meaningful. Instead, what matters is whether a researcher will allow for interactions between the independent variables. The calculations of the model sets are therefore the same as if there was a variable of interest, but excluding the sets that have HCI in it. This suggests that considering the restriction that main effects should always follow along with interactions, the sets of interest here are ME and ME + CCI. With no such restriction, the sets of interest are ME, CCI, and ME + CCI. The number of models with and without restrictions, different number of variables, and how to calculate them can be found in Table S9 and Table S10. 


 
