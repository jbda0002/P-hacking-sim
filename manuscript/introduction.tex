Imagine that you are interested in the effect of a specific variable A (the independent variable) on another variable B (the dependent variable). You are not sure whether A may interact with other variables, so you decide to collect several covariate variables you think could influence your presumed causal chain. After collecting the data, you run multiple models with and without these additional variables as covariates in order to probe the data; you remove some outliers to improve the clarity of the results; and you finally find that your independent variable significantly interacts with one of your covariates. Hopefully by now, most researchers are aware that this research process has an increased chance of resulting in a false positive finding (i.e., Type I error) and a low chance of replicability. For instance, simulations suggest that running even a small number of models with just one covariate and its interaction, dropping conditions and having multiple dependent variables can increase the chance of a false-positive finding to 60.7\% \citep{Simmons2011}.\\

It is, therefore, advisable that researchers plan and conduct their research differently; before collecting any data, they should preregister all variables, hypotheses, sample size justification, outlier criteria, and analyses they aim to run. As everything is preregistered, the researcher may only run one model to test each hypothesis. Additional models or tests can still be performed and published, but are then labeled as exploratory \citep{Nosek2018}. Following this procedure, one can expect to increase the replicability of research findings by lowering the risk of false positives to any given preset alpha level (such as the conventional level of statistical significance of 5\%) \citep{Moore2016}. But can researchers rely on this procedure to entirely eliminate the increased rate of false-positive findings? \\

The 5\% chance of a false positive result is an assumption that relies on running only one single hypothesis test and that all statistical analysis are done correctly. There are several studies showing that misspecifying a statistical model (i.e., the true model is not contained in the model set or wrong distributional assumptions are being made) can increase the false positive to above the alpha level even when running a single model \citep{Dennis2019,Litiere2007}. Misspecification is not the only challenge; noise in measured independent variables (e.g., individuals overstating their height or socioeconomic status) can also increase the risk of false positives \citep{Brunner2009} and so can variables with ceiling effects (e.g., a large proportion of participants scores toward the high end of the scale) \citep{Austin2003}. Austin and Brunner also showed that the rate of false positive findings increases when having an interaction effect with a naturally continuous covariate which has been dichotomized \citep{Austin2004}. The problem with such dichotomization is further aggravated when the sample size increases, which runs counter to the recommendation to use larger samples to minimize the risk of Type I error \citep{simmons2018}. 
\\
It may appear tempting to dismiss these findings as irrelevant as long as we rely on sound scientific and statistical practices. However, it is easy to determine if a model is misspecified in a simulation study since we know the ground truth about the data generating process, but in reality we hardly ever know this. Can we judge the risk of model misspecification simply from the model? This might be possible in some cases such as when estimating the interaction effect, but omitting the corresponding main effect. Omitting the main effect leads to a bias of the coefficients \citep{Branbor2006} which could consequently lead to a higher rate of false positives. The practice of estimating interactions without the corresponding main effects is widespread \citep{Branbor2006} and some method books still suggest doing this \citep{Cleves2008}. This is justified by arguing that if the model is based on theory, this is an acceptable practice \citep{aiken1991multiple}. To the best of our knowledge, no studies have examined whether this practice leads to an increase in false positive findings. The same could hold for many other common practices. For instance, running models using different criteria for outlier deletion or several dependent variables (e.g., when the researcher is not sure about how to measure the construct) could in principal increase false positives due to multiple testing. However, it has also been shown that running only one model with outlier deletion increases the rate of false positives if the exclusion leads to range restriction in the dependent or independent variable \citep{Raju2003}. \\        

Due to the arguments outlined above, we believe there is a need for a more nuanced understanding regarding how different model specifications and factors in the analysis can increase the risk of false-positive findings. There are already simulation studies examining the false-positive probability (FPP) \citep{Simmons2011}, which is the chance of erroneously finding one or more significant effects among all the tested ones. However, while the FPP can reveal problems with conducting multiple tests and with flexible research practices in general, it does not provide information on how many of the tested models contain a significant effect. To understand this latter aspect, we rather need to examine the false positive ratio (FPR), hereinafter defined as the number of models with a significant effect within any given set of models. Both measurements can say something about a specific model specification and factors in the analysis; the FPP answers what the likelihood is that a researcher can find a significant effect when deliberately looking for it, whereas the FPR answers what the risk of getting a false positive is even when the researcher is not deliberately trying to obtain a significant effect (e.g., when preregistering a research plan). \\

In the present article, we report the results of a simulation study testing the effect of a set of researcher flexibilities - here defined as a list of flexibilities the researcher can choose from when designing and analyzing a study - on the FPP and FPR. A simulation approach is useful here since it is not straightforward how to calculate the FPP and FPR when the different flexibilities interact with each other. By examining both metrics, we bring together two streams of literature: one concerned with researcher flexibilities\footnote{This also has other names, such as researcher degrees of freedom \citep{Simmons2011}, p-hacking \citep{simonsohn2014p}, data snooping \citep{white2000reality}, data mining \citep{lovell1983}, and data fishing \citep{selvin1966data}.} and the other one concerned with type I error rates due to specific conditions such as model misspecification. We believe that both perspectives are necessary to advance science in terms of replicability since open science practices may not be a sufficient safeguard against false-positive findings; some methods and analyses could inflate false positive rates even when preregistered. In our simulation study, we included some of the flexibilities previously examined by \cite{Simmons2011}\footnote{We did not include the flexibility regarding reporting subsets of experimental conditions because we focused on two basic data structures, namely binary and continuous variables.} and questionable research practices observed in the fields of social psychology \citep{Cairo2020} and management \citep{OBoyle2017}. These include using several covariates, having interaction effects with and without main effects, using several dependent variables, different outlier criteria, and larger samples.  
Our results show that finding a significant effect is not difficult, but only a matter of testing enough models as the FPP can reach just below 100\% using different flexibilities. More importantly, our results also show that simply preregistering a research plan does not bound false positive findings to 5\% as the FPR is higher than that when using interaction effects due to the multiple testing. The FPR rises even more when having interactions without including main effects and using binary variables. Finally, a bigger sample does not help with reducing the rate of false positive findings, and in some cases even makes it worse. 
