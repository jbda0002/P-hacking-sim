Imagine that you are interested in the effect of a key independent variable on a dependent variable. You are not sure whether your key variable may interact with other variables, so you decide to collect several covariate variables you think could influence your presumed causal chain. After collecting the data, you run multiple models with and without the additional variables as covariates in order to probe the data; you remove some outliers to improve the clarity of the results; and you finally find that your key variable significantly interacts with one of your covariates. Hopefully, most researchers are aware that such a research process has an increased chance of resulting in a false-positive result and a low chance of replicability. For instance, simulations suggest that running even a small number of models with just one covariate and its interaction, dropping conditions and having multiple dependent variables can increase the chance of a false-positive result to 60.7\% \citep{Simmons2011}.\\

It is, therefore, advisable that researchers plan and conduct their research differently; before collecting any data, they should preregister all variables, hypotheses, sample size justification, outlier criteria, and analyses they aim to run. As everything is preregistered, a researcher may only run one model to test each hypothesis. Additional models or tests can still be performed and published, but are then labeled as exploratory \citep{Nosek2018}. Following this procedure, one should in principle expect to increase the replicability of research results by lowering the risk of false-positives to a preset alpha level such as the conventional level of statistical significance of 5\% \citep{Moore2016}. But can researchers rely on this procedure to entirely eliminate the increased rate of false-positive results? Probably not. \\

The 5\% chance of a false-positive result is an assumption that relies on running only one single hypothesis test and that all statistical analysis are done correctly. Previous studies have shown that misspecifying a statistical model i.e., the true model is not contained in the model set or wrong distributional assumptions are being made, can increase false positives to above the alpha level even when running a single model \citep{Dennis2019,Litiere2007}. Misspecification is not the only challenge; noise in measured independent variables e.g., individuals overstating their height or socioeconomic status, can also increase the risk of false positives \citep{Brunner2009} and so can variables with ceiling effects i.e, a large proportion of participants scores toward the high end of the scale \citep{Austin2003}. Austin and Brunner have also showed that the rate of false-positive results increases when having an interaction effect with a naturally continuous covariate which has been dichotomized \citep{Austin2004}. The problem with such dichotomization is further aggravated when the sample size increases, which runs counter to the recommendation to use larger samples to minimize the risk of Type I error \citep{simmons2018}. 
\\
It may appear tempting to dismiss these findings as irrelevant as long as we rely on sound scientific and statistical practices. However, it is easy to determine if a model is misspecified in a simulation study since we know the ground truth about the data generating process, but in reality we hardly ever know this. Can we judge the risk of model misspecification simply from the model? This might be possible in some cases such as when estimating the interaction effect, but omitting the corresponding main effect. Omitting the main effect leads to a bias in the coefficients \citep{Branbor2006} which could consequently lead to a higher rate of false positives. The practice of estimating interactions without the corresponding main effects is widespread \citep{Branbor2006} and some method books still suggest doing this \citep{Cleves2008}. This is justified by arguing that if the model is based on theory, this is an acceptable practice \citep{aiken1991multiple}. To the best of our knowledge, no studies have examined whether this practice leads to an increase in false-positive results. The same could be true for many other common research practices. For instance, running models using different criteria for outlier deletion or testing several dependent variables should in principle increase false-positives due to multiple testing. However, it has also been shown that running even a single model with outlier deletion can increase the rate of false-positives if the exclusion leads to range restriction in the dependent or independent variable \citep{Raju2003}. \\        

Due to these considerations, we believe there is a need for a more nuanced understanding of how different model specifications and factors in the analysis can increase the risk of false-positive results. There are already simulation studies examining the false-positive probability \citep{Simmons2011}, which is the chance of erroneously finding one or more significant effects among all the tested ones. However, while the false-positive probability can reveal problems with conducting multiple tests and with flexible research practices in general, it does not provide information on how many of the tested models contain a significant effect. For example, with one key variable and three covariates it is possible to test 95 models. If just one of these models produces a false-positive then the false-positive probability is 100\%. However, the risk of running that one offending model is rather small and would require a concerted effort to test a large number of the possible models. On the other hand, if 25 of the 95 models produce false-positives the risk is much higher. To understand this latter aspect, we need to examine the false-positive ratio, which we define as the number of models with a significant key variable effect within a given set of models. Both measurements provide information about the the risk of false-positives, but under different conditions: the false-positive probability inform us about the likelihood that a researcher can generate a false-positive result when intentionally testing a large number of models, whereas the false-positive ratio inform us about the risk of producing a false-positive when testing a single model such as when following a preregistered research plan. \\

In this article, we report the results of a simulation study on the false-positive probability and false-positive ratio given a set of researcher flexibilities - here defined as a list of possible approaches a researcher can choose from when designing and analyzing a study. A simulation approach is useful here since it is not straightforward how to calculate the false-positive probability and false-positive ratio when the different flexibilities interact with each other. By examining both metrics, we bring together two streams of literature: one concerned with researcher flexibilities\footnote{This also has other names, such as researcher degrees of freedom \citep{Simmons2011}, p-hacking \citep{simonsohn2014p}, data snooping \citep{white2000reality}, data mining \citep{lovell1983}, and data fishing \citep{selvin1966data}.} and the other one concerned with type I error rates due to specific conditions such as model misspecification. We believe that both perspectives are necessary to advance science in terms of replicability since open-science practices may not be a sufficient safeguard against false-positive results; some methods and analyses could inflate false-positive rates even when preregistered. In our simulation study, we include some of the flexibilities previously examined by \cite{Simmons2011}\footnote{We did not include the flexibility regarding reporting subsets of experimental conditions because we focused on two basic data structures, namely binary and continuous variables.} and questionable research practices observed in the fields of social psychology \citep{Cairo2020} and management \citep{OBoyle2017}. These include using several covariates, having interaction effects with and without main effects, using several dependent variables, different outlier criteria, and larger samples. \\ 
Our results show that producing a false-positive effect is not difficult, but merely a matter of testing enough models as the false-positive probability can reach just below 100\% when using different flexibilities. More importantly, our results also show that simply preregistering a research plan does not bind the false-positive ratio to 5\% as many models have inflated false-positive rates. Finally, a larger sample does not help reduce the rate of false-positive results, and in some cases even makes it worse. 
