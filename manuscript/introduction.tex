Imagine that you are interested in the effect of a key independent variable on a dependent variable. You are not sure whether your key variable may interact with other variables, so you decide to collect several covariate variables you think could influence your presumed causal chain. After collecting the data, you run multiple models with and without the additional variables as covariates in order to probe the data; you remove some outliers to improve the clarity of the results; and you finally find that your key variable significantly interacts with one of your covariates.
Even though, that it might seem intuitive that this kind of practice may increase false-positive findings, there seem to be a problem in some research communities with understanding the consequences of such flexibilities \citep{Makel2021,John2012,Agnoli2017,fraser2018}. 
Simulations have shown that running even a small number of models with just one covariate and its interaction, dropping conditions and having multiple dependent variables can increase the chance of a false-positive result to 60.7\% \citep{Simmons2011}, however, these questionable research practices are presumably pervasive.
Researcher flexibilities have been studied for many years and under different names, such as researcher degrees of freedom \citep{Simmons2011}, p-hacking \citep{simonsohn2014p}, data snooping \citep{white2000reality}, data mining \citep{lovell1983}, and data fishing \citep{selvin1966data}, but have received increasing attention with the open science movement \citep{banks2019answers}.   \\

Open science practices forces researchers to plan and conduct their research differently; before collecting any data, they should preregister all variables, hypotheses, sample size justification, outlier criteria, and analyses they aim to run \citep{Simmons2020, VANTVEER20162}. As everything is preregistered, a researcher may only run one model to test each hypothesis. Additional models or tests can still be performed and published, but are then labeled as exploratory \citep{Nosek2018}. Following this procedure, one should in principle expect to increase the replicability of research results by lowering the risk of false-positives to a preset alpha level such as the conventional level of statistical significance of 5\% \citep{Moore2016}. But can researchers rely on this procedure to entirely eliminate the increased rate of false-positive results? Probably not. \\

The 5\% chance of a false-positive result is an assumption that relies on running only one single hypothesis test and that all statistical analysis are done correctly. Previous studies have shown that misspecifying a statistical model i.e., the true model is not contained in the model set or wrong distributional assumptions are made, can increase false-positives to above the alpha level even when running a single model \citep{Dennis2019,Litiere07}. Misspecification is not the only challenge; noise in measured independent variables e.g., individuals overstating their height or socioeconomic status, can also increase the risk of false-positives \citep{Brunner2009} and so can variables with ceiling effects i.e, a large proportion of participant scores toward the high end of the scale \citep{Austin2003}. Furthermore, it has also been shown that the rate of false-positive results increases when having an interaction effect with a naturally continuous covariate which has been dichotomized \citep{Austin2004, maxwell1993bivariate}. The problem with such dichotomization is further aggravated when the sample size increases, which runs counter to the recommendation to use larger samples to minimize the risk of Type I errors \citep{simmons2018}. 
\\
It may appear tempting to dismiss these findings as irrelevant as long as we rely on sound scientific and statistical practices. However, it is easy to determine if a model is misspecified in a simulation study since we know the ground truth about the data generating process, but in reality we hardly ever know this. Can we judge the risk of model misspecification simply from the model? This might be possible in some cases such as when estimating the interaction effect, but omitting the corresponding main effect. Omitting the main effect leads to a bias in the coefficients \citep{Branbor2006} which could consequently lead to a higher rate of false-positives. The practice of estimating interactions without the corresponding main effects is widespread \citep{Branbor2006} and some method books still suggest doing this in some cases \citep{Cleves2008}. The argument is that omitting main effects is sometimes justified based on theory \citep{aiken1991multiple}. The same could be true for many other common research practices. For instance, testing models using different criteria for outlier deletion or testing several dependent variables should in principle increase false-positives due to multiple testing. However, it has also been shown that running even a single model with outlier deletion can increase the rate of false-positives if the exclusion leads to range restriction in the dependent or independent variable \citep{Raju2003}.  \\        

To conclude, the risk of false-positive results can be inflated due to two main reasons, either from testing multiple models using researcher flexibilities or due to testing a single model under conditions such as misspecification. The open science movement has largely focused on researcher flexibilities and the risk of testing multiple models whereas traditional statistical research has focused on conditions that can inflate Type I error rates when testing a single model. Judging from previous literature, both perspectives are relevant to understanding and reducing false-positive findings and integrating them may help advance our understanding of this important topic. A direct comparison may, for instance, reveal if false-positive results are more likely to arise from researchers testing multiple models or from researchers testing a single model under certain conditions. Type I error rates can be inflated by more or less common conditions such omitting main effects in interaction models, dichotomizing variables, or using outlier deletion, but how large is the risk compared to that of researcher flexibilities? In this paper, we jointly examine the risk of false-positives posed by testing multiple models and by testing a single model depending on the model specification. We define two outcome measures; the \textit{false-positive probability} and the \textit{false-positive ratio}. \\
Previous research on researcher flexibilities has examined the false-positive probability \citep{Simmons2011}, which is the chance of erroneously finding one or more significant effects among all the tested ones. The false-positive probability reveals problems with conducting multiple tests and with flexible research practices in general, but it does not provide information on how many of the tested models contain a significant effect. For example, with one key variable and three covariates it is possible to test 95 models\footnote{Equations for calculating this number is shown later in the paper}. If just one of these models produces a false-positive then the false-positive probability is 100\%. However, the risk of running that one offending model is rather small and would require a concerted effort to test a large number of the possible models. On the other hand, if 25 of the 95 possible models produce false-positives the risk is much higher. To understand this latter aspect, we examine the false-positive ratio, which we define as the number of models with a significant key variable effect within a given set of models. Both measurements provide information about the the risk of false-positives, but under different conditions: the false-positive probability inform us about the likelihood that a researcher can generate a false-positive result when testing a large number of models, whereas the false-positive ratio informs us about the risk of producing a false-positive result when testing a single model such as when following a preregistered research plan. \\

In this article, we report the results of a simulation study on the false-positive probability and false-positive ratio for different types of regression models. 
To this end, we first develop a combinatorial analysis of all possible regression models that a researcher can test with a single variable of interest and a given number of covariates. We show how the total set of all possible models can be decomposed into seven subsets e.g., the set of models with main effects only, the set of models with interaction effects between variable of interest and covariates, the set of models with interactions between covariates, and the sets that combine sets.  
In our simulation, we examine the models sets separately and under different conditions such as changes in the correlation between the covariates and the dependent variable, different types of data structure (binary and continuous variables), and larger sample sizes.
Our results show that producing a false-positive effect is not difficult, but merely a matter of testing enough models as the false-positive probability can reach just below 100\% in some model sets. More importantly, our results show that testing a single preregistered model does not bind the false-positive ratio to 5\% as many model sets have an inflated false-positive ratio. Finally, a larger sample does not help reduce the risk of false-positive results, and in some cases even makes it worse.
Our analysis brings together two streams of literature: one concerned with researcher flexibilities and the other one concerned with type I error rates due to specific conditions such as model misspecification. Our results show that both perspectives are necessary to advance science in terms of replicability since current open-science practices may not be a sufficient safeguard against false-positive results; some types of analyses can inflate the risk of false-positive results even when preregistered. 
 \\ 
 
