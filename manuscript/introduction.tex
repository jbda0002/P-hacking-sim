You are interested in the effect of a specific independent variable on a dependent variable. You are not sure how the independent variable may interact with other variables, so you decide to collect several variables you think it could interact with. After collecting the data you run models with and without the co-variates to probe the data, remove some outliers to improve the clarity of the results and find that your hypothesized variable has a significant interaction with one of the covariates. Hopefully, by now most researchers are aware that this research process is likely to result in false positive findings and low chances of replicability. As a consequence we now plan and conduct our research differently: Before we collect any data, we preregister our study and all variables and hypotheses we want to test. As everything is preregistered, we only run one model to test each hypothesis. Additional models or tests can be published, but are always labelled as exploratory. Following this procedure we expect to increase the reproducibility of our findings by lowering the risk of false positive findings to the preset alpha level of 5\%. But can we rely on this procedure to eliminate false positive findings? \\

There is no doubt that there are many issues with analyzing the data as in the first example, as running multiple models will increase the probability of false positive results. Simulations suggest that running even a small number of models with different variables, and dropping conditions  etc can increase the chance of a false positive result to 81.5\% \citep{Simmons2011}. The Open Science solution to this problem is that the researcher commits to only performing one analysis, but with the possibility to label other analyses that goes beyond the plan as exploratory \citep{Nosek2018}. This procedure should lower the false positive for the preregistered study to a predetermined alpha level, normally set at 5\% \citep{Moore2016}. However, the 5\% chance of a false positive result is an assumption that relies on only running one single hypothesis test and that all statistics are done correctly. There are several studies showing that misspecifying a  statistical model can increase the false positive to above the alpha level even when only running one single model \citep{Dennis2019,Litiere2007}. Misspecification is not the only challenge, noise in measured independent variables can also increase the risk of false positives \citep{Brunner2009} and so can variables with ceiling effects \citep{Austin2003}. Austin and Brunner also show that interaction effects with a naturally continuous co-variate when the co-variate is dichotomized increases false positive findings \citep{Austin2004}. The problem with dichotomization is further aggravated when the sample size increases, which runs counter to the recommendation to use larger samples to minimize the risk of false positive findings \citep{Simmons2011}. It may appear tempting to dismiss these findings as irrelevant as long as we rely on sound scientific and statistical practices. However, it is easy to determine if a model is misspecified in a simulation study since we know the ground truth about the data generating process, but in reality we never know this. Can we judge the risk of misspecification simply from the model? Perhaps we can, by looking for issues such as estimating interaction terms, but omitting the corresponding main effects. By omitting the main effects, this is the same as setting the main effect to zero and can lead to part of the variance to be captured in the error term which will then be correlated with the interaction term \citep{Branbor2006}. The practice of estimating interactions without the corresponding main effects is widespread \citep{Branbor2006}, and some method books still suggest doing this \citep{Cleves2008}. This is done with the argument if just the model is based in theory this is an acceptable practice \citep{aiken1991multiple}. To the best of our knowledge no studies have examined whether this practice leads to an increase in false positives. The same is true for many other common practices. Clearly, running models with and without outlier deletion increases false positives due to multiple testing, but it is also possible that only running one model with outlier deletion could increase false positives if the exclusion leads to range restriction in the dependent or independent variable \citep{Raju2003}. \\        

We believe these findings show a need for a better understanding of how different specifications and factors in the analysis can increase the risk of false positive results. There are already simulation studies examining what can be called the false positive probability (FPP) \citep{Simmons2011}. We define this probability as the chance of finding one or more significant models of all the models tested. While the false positive probability can reveal problems with conducting multiple models and with flexible research practices in general, it does not, however, clarify whether a particular combination of methods is more likely than others to produce a false positive result. To understand this, we instead need to examine the false positive ratio (FPR). With this we mean, out of a given set of models a researcher can choose from what is the probability that the choosen model produces a false positive result. \\

In this article we report the results of a simulation study that jointly examines the false positive probability and the false positive rate. A simulation approach is useful since it is not obvious how to calculate the false positive probability and false positive ratio when the different factors interact with each other. By examing both metrics we bring together two strands of literature: one concerned with data phising and researcher flexibilities and another concerned with type I error rates due to specific conditions such as model misspecification. We believe both perspectives are necessary to advance science and replicability since open science practices cannot be a sufficient safeguard against false positive findings â€“ some models and methods could have inflated false positive rates even though they are preregistered. In our simulation study we include some of the factors previous examine by \cite{Simmons2011} and flexibilities identified by \cite{Wicherts2016}. These include expanding the model with more covariates, including interactions both between the variable of interest, but also between the covariates themselves. When including interactions, we examine what happens when the main effect is included or not. We also examine the effect of the most commonly used outlier criteria in the literature \citep{Leyes2013} and the effect of using several correlated dependent variables. Simmons and colleagues recommended to increase the sample size to overcome some of the problem with false positive findings \citep{Simmons2011}, and we therefore also examine how an increase in sample size affects the false positive probability and false positive ratio. 
