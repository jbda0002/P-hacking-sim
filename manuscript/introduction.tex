Imagine that you are interested in the effect of a specific variable A (the independent variable) on another variable B (the dependent variable). You are not sure whether A may interact with other variables, so you decide to collect several covariate variables you think could influence your presumed causal chain. After collecting the data, you run multiple models with and without these additional variables as covariates in order to probe the data; you remove some outliers to improve the clarity of the results; and you finally find that your independent variable significantly interacts with one of your covariates. Hopefully by now, most researchers are aware that this research process is likely to result in a false positive finding (i.e., Type I error) and a low chance of replicability. Consequently, it is advisable that researchers plan and conduct their research differently; before collecting any data, they should preregister their study and all variables and hypotheses they aim to test. As everything is preregistered, the researcher may only run one model to test each hypothesis. Additional models or tests can still be performed and published, but are then labelled as exploratory. Following this procedure, one can expect to increase the replicability of research findings by lowering the risk of false positives to any given preset alpha level (such as the conventional level of statistical significance of 5\%). But can researchers rely on this procedure to entirely eliminate false positives? \\

There is no doubt that there are many issues with analyzing the data as in the first example, as running multiple models will increase the probability of false positive results. Simulations suggest that running even a small number of models with different variables, dropping conditions etc. can increase the chance of a false positive result to 81.5\% \citep{Simmons2011}. The Open Science solution to this problem is that the researcher commits to performing only one analysis, with the possibility to label other analyses that go beyond the plan as exploratory \citep{Nosek2018}. This procedure should lower the false positive for the preregistered study to a predetermined alpha level, normally set to 5\% \citep{Moore2016}. However, the 5\% chance of a false positive result is an assumption that relies on running only one single hypothesis test and that all statistics are done correctly. There are several studies showing that misspecifying a  statistical model can increase the false positive to above the alpha level even when only running one single model \citep{Dennis2019,Litiere2007}. Misspecification is not the only challenge; noise in measured independent variables can also increase the risk of false positives \citep{Brunner2009} and so can variables with ceiling effects \citep{Austin2003}. Austin and Brunner also show that the rates of false positive findings increase when having the interaction effects with a naturally continuous covariate which is dichotomized \citep{Austin2004}. The problem with such dichotomization is further aggravated when the sample size increases, which runs counter to the recommendation to use larger samples in order to minimize the risk of Type I error \citep{Simmons2011}. 
\\
It may appear tempting to dismiss these findings as irrelevant as long as we rely on sound scientific and statistical practices. However, it is easy to determine if a model is misspecified in a simulation study since we know the ground truth about the data generating process, but in reality we hardly ever know this. Can we judge the risk of misspecification simply from the model? Perhaps we can, by looking at issues such as estimating interaction terms, but omitting the corresponding main effects. By omitting the main effects, which is the same as setting the main effect to zero, can lead to part of the variance to be captured in the error term which will then be correlated with the interaction term \citep{Branbor2006}. The practice of estimating interactions without the corresponding main effects is widespread \citep{Branbor2006} and some method books still suggest doing this \citep{Cleves2008}. This is justified with arguing that if just the model is based on theory, this is an acceptable practice \citep{aiken1991multiple}. To the best of our knowledge, no studies have examined whether this practice leads to an increase in false positives findings. The same is true for many other common practices. Clearly, running models with and without outlier deletion increases false positives due to multiple testing, but it is also possible that running only one model with outlier deletion could increase false positives if the exclusion leads to range restriction in the dependent or independent variable \citep{Raju2003}. \\        

We believe there is a need for a more nuanced understanding regarding how different specifications and factors in the analysis can increase the risk of false positives. There are already simulation studies examining what the false positive probability (FPP) \citep{Simmons2011}, which can be thought of as the chance of erroneously finding one or more significant models among all the tested ones. However, while the FPP can reveal problems with conducting multiple tests and with flexible research practices in general, it does not clarify whether a particular combination of methods is more likely than others to produce a false positive. To understand this latter aspect, we rather need to examine the false positive ratio (FPR), hereinafter defined as the probability that a selected model produces a false positive out of any given set of models from which a researcher can choose. \\

In the present article, we report the results of a simulation study that examines the FPP and FPR, respectively. A simulation approach is useful since it is not obvious how to calculate the false positive probability and false positive ratio when the different factors interact with each other. By examing both metrics we bring together two strands of literature: one concerned with p-hacking\footnote{This also has other names, such as researchers degrees of freedom \citep{Simmons2011}, data snooping \citep{white2000reality}, data mining \citep{lovell1983}, and data fishing \citep{selvin1966data} .} \citep{simonsohn2014p} and researcher flexibilities and another concerned with type I error rates due to specific conditions such as model misspecification. We believe both perspectives are necessary to advance science and replicability since open science practices may not be a sufficient safeguard against false positive findings â€“ some models and methods could have inflated false positive rates even though they are preregistered. In our simulation study we include some of the factors previous examine by \cite{Simmons2011} and flexibilities identified by \cite{Wicherts2016}. These include expanding the model with more covariates, including interactions both between the variable of interest, but also between the covariates themselves. When including interactions, we examine what happens when the main effect is included or not. We also examine the effect of the most commonly used outlier criteria in the literature \citep{Leyes2013} and the effect of using several correlated dependent variables. Simmons and colleagues recommended to increase the sample size to overcome some of the problem with false positive findings \citep{Simmons2011}, and we therefore also examine how an increase in sample size affects the false positive probability and false positive ratio. 
